{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8486eb71-b2fc-4249-a9d1-3eb1988bc1d2",
   "metadata": {},
   "source": [
    "### Imports and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84797fad-600c-492c-a402-167a3194654a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version:  2.4.1\n",
      "tfds version:  4.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('tensorflow version: ', tf.__version__)\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "print('tfds version: ', tfds.__version__)\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime as datetime\n",
    "from IPython import display\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b85270-f1fd-43b0-a583-c3a9e0bddcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n",
      "GPUs:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e775c-67ff-45eb-9874-7ce1de673cc4",
   "metadata": {},
   "source": [
    "### Ways to iterate over the dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3bd65e10-383c-4211-8d54-b8c70803dcdc",
   "metadata": {},
   "source": [
    "# Dictionary\n",
    "\n",
    "ds = ds.take(1)  # Only take a single example\n",
    "\n",
    "for example in ds:  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "    print(list(example.keys()))\n",
    "    image = example[\"image\"]\n",
    "    label = example[\"attributes\"]\n",
    "    print(image.shape, label)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "289fac94-6260-4492-a861-d91ad1eac75d",
   "metadata": {},
   "source": [
    "# Show examples\n",
    "\n",
    "ds, info = tfds.load('celeb_a', split='train', shuffle_files=True, with_info=True)\n",
    "fig = tfds.show_examples(ds, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f04941-4bfa-4ee6-8058-66d03e6257a2",
   "metadata": {},
   "source": [
    "### Load and Prepare the Dataset / Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9ab004-87d9-46dc-94e1-6b98c25b53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/datasets/keras_example\n",
    "\n",
    "(ds_train_raw, ds_test_raw), ds_info = tfds.load(\n",
    "    'celeb_a',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,)\n",
    "\n",
    "def preprocess(data):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    image = data['image']\n",
    "    label = data['attributes']\n",
    "    NUM_BOXES = 1\n",
    "    boxes = [[.1,.1,.9,.9,]]\n",
    "    box_indices = tf.zeros(shape=(NUM_BOXES,), dtype=tf.int32)\n",
    "    #image = tf.image.resize(image[20:-20], [64, 64], antialias=True)\n",
    "    image = tf.image.crop_and_resize(tf.expand_dims(image, axis=0), crop_size=[64, 64], boxes=boxes, box_indices=box_indices)\n",
    "    image = tf.cast(image, tf.float32) / 255. # normalise images to between 0 and 1\n",
    "    image = image[0]\n",
    "    return image, label\n",
    "\n",
    "ds_train = ds_train_raw.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128, drop_remainder=True)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c0d3f71-1878-48c7-924b-1cb70ff4a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in ds_train:\n",
    "#     for i in range(10):\n",
    "#         plt.imshow(data[0][i])\n",
    "#         plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a0b1f7b-390f-442c-bb1d-0aebb54ab5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 64, 64, 3)\n",
      "(64, 64, 3)\n",
      "dict_keys(['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young'])\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Get attributes list\n",
    "for images, attributes in ds_train:\n",
    "    break\n",
    "    \n",
    "# Checks\n",
    "print(images.shape)\n",
    "print(images[0].shape)\n",
    "print(attributes.keys())\n",
    "print(images.numpy().max())\n",
    "print(images.numpy().min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48fa7f2-d400-4205-a90f-8a4df7447bdd",
   "metadata": {},
   "source": [
    "## Create the models\n",
    "\n",
    "Adapated from https://www.tensorflow.org/tutorials/generative/dcgan which uses MNIST to create a non-conditional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ab8cd-ac07-42bb-8aa6-a43131505c28",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "The generator uses tf.keras.layers.Conv2DTranspose (upsampling) layers to produce an image from a seed (random noise). Start with a Dense layer that takes this seed as input, then upsample several times until you reach the desired image size of 64x64x1. Notice the tf.keras.layers.LeakyReLU activation for each layer, except the output layer which uses tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c81effbe-eff2-479e-bbc5-a4db0c90618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator\n",
    "def make_generator_model():\n",
    "    \n",
    "    #input_attributes = layers.Input(shape=(40,))\n",
    "    #input_noise = tf.random.normal([128, 24])\n",
    "    #merge = layers.Concatenate()([input_attributes, input_noise])\n",
    "    \n",
    "    model = tf.keras.Sequential(name='Generator')\n",
    "    # add noise layer\n",
    "    #model.add(layers.GaussianNoise(stddev=0.1, input_shape=(40,)))\n",
    "    \n",
    "    # Densly connection NN layer of 8*8*256 (16,384) neurons. This is taking in the input and multiplying with the weights\n",
    "    model.add(layers.Dense(8*8*256, use_bias=False)) \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # 8 x 8 image with 256 features.\n",
    "    model.add(layers.Reshape((8, 8, 256)))\n",
    "    #assert model.output_shape == (None, 8, 8, 256)  # Note: None is the batch size\n",
    "\n",
    "    # Compress channels/features to 128\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)) # channels=128, filter size=5*5\n",
    "    #assert model.output_shape == (None, 8, 8, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Upsampling block\n",
    "    model.add(layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    #assert model.output_shape == (None, 16, 16, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    # Upsampling block\n",
    "    model.add(layers.Conv2DTranspose(32, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    #assert model.output_shape == (None, 32, 32, 32)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(16, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    # Output layer - Activation of sigmoid is used to squash values between 0 and 1 in line with image values after preprocessing\n",
    "    model.add(layers.Conv2D(3, (3,3), activation='sigmoid', padding = 'same', use_bias=False))\n",
    "    #assert model.output_shape == (None, 64, 64, 3)\n",
    "    \n",
    "    #output = model(merge)\n",
    "    \n",
    "    #model = tf.keras.Model(inputs=[input_attributes], outputs=output, name='Generator_2')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f9ce0e8-353e-481b-b83b-390499839798",
   "metadata": {},
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "generated_image = generator(attributes_noise, training=False)\n",
    "print(generated_image.dtype)\n",
    "print(generated_image.shape)\n",
    "\n",
    "plt.imshow(generated_image[0])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(generated_image[1])\n",
    "plt.show()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a7e409-cdd3-436b-b6a6-d7a8dbf41783",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "The discriminator is a CNN-based image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d73e3f30-8cf1-4c3c-8fca-cf5fe1a03540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    \n",
    "    # label input\n",
    "    attribute_vector = layers.Input(shape=(40+24))\n",
    "    input_image = layers.Input(shape=(64,64,3))\n",
    "    \n",
    "    conv_model = tf.keras.Sequential(name='Discriminator_conv_model')\n",
    "    \n",
    "    # gan hacks - add noise to help generate a variety of images\n",
    "    conv_model.add(layers.GaussianNoise(stddev=0.1))\n",
    "    \n",
    "    # downsample - increase filters as it progresses through the model\n",
    "    conv_model.add(layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same'))\n",
    "    conv_model.add(layers.BatchNormalization())\n",
    "    conv_model.add(layers.LeakyReLU())\n",
    "    #assert conv_model.output_shape == (None, 32, 32, 64)\n",
    "    \n",
    "    #downsample\n",
    "    conv_model.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same'))\n",
    "    conv_model.add(layers.BatchNormalization())\n",
    "    conv_model.add(layers.LeakyReLU())\n",
    "    \n",
    "    #downsample\n",
    "    conv_model.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same'))\n",
    "    conv_model.add(layers.LeakyReLU())\n",
    "    conv_model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # classifier - flatten to 1D array\n",
    "    conv_model.add(layers.Flatten())\n",
    "    \n",
    "    # Push image through model above, shape=[batch,dimensions(128*8*8)]\n",
    "    img_feature_vec = conv_model(input_image)\n",
    "    \n",
    "    # concatenate layers - the image vector + the attribute labels, shape=[batch, dimensions(128*8*8+40)]\n",
    "    merge = layers.Concatenate()([img_feature_vec, attribute_vector])\n",
    "    \n",
    "    # Gan Hacks - deeper model\n",
    "    dense_model = tf.keras.Sequential(name='Discriminator_deep')\n",
    "    dense_model.add(layers.Dense(128))\n",
    "    #dense_model.add(layers.Dense(256))\n",
    "    dense_model.add(layers.LeakyReLU())\n",
    "    dense_model.add(layers.Dropout(0.3))\n",
    "    dense_model.add(layers.Dense(1))\n",
    "\n",
    "    # Push image + attributes through model and make a prediction\n",
    "    output = dense_model(merge)\n",
    "    \n",
    "    # Create new model here with correct shapes which need to be defined as inputs\n",
    "    model = tf.keras.Model(inputs=[input_image, attribute_vector], outputs=output, name='Discriminator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89f74ea2-8b35-49f2-ae72-2fd1f4002425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Models\n",
    "# Classify the generated images as real or fake using the discriminator.\n",
    "# Model is trained to output positive values for real images and negative values for fake images\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "#decision = discriminator((generated_image, attributes_noise))\n",
    "#print(decision.shape)\n",
    "#print(decision[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c572e1-6fe8-4b65-9ed0-caddc810f2fb",
   "metadata": {},
   "source": [
    "## Define the loss and optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "777205f3-1f15-48fe-a983-d902fe085cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "# The discriminator has binary input (fake or real data) so a binary loss function is used\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aaea0a-313c-4cdb-be79-6160dfd8723b",
   "metadata": {},
   "source": [
    "### Discriminator loss\n",
    "\n",
    "Quantifies how well the discriminator is able to distinguish real images from fakes.  \n",
    "\n",
    "It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "692afa5b-8b3e-4b47-91ef-ed50b6a408bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b1be0-18b8-4cb2-bf2d-0049a4e996ed",
   "metadata": {},
   "source": [
    "### Generator loss\n",
    "\n",
    "Quantifies how well it was able to trick the discriminator.  \n",
    "\n",
    "If the generator is performng well, the discriminator will classify the fake images as real (or 1). This function compares the discriminators decisions on the generated images to an array of 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b9ac231-f711-436d-95fe-2ada50c54162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf04dc-1cc1-4eee-9fe6-7ebe497ddbd3",
   "metadata": {},
   "source": [
    "### Optimisers\n",
    "\n",
    "Two different optimisers required since the two networks are trained separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d05ee072-92c9-4553-b0f8-80014a9d4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed13888-a451-4fd2-8dba-3e9e05824c48",
   "metadata": {},
   "source": [
    "### Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cfaf80e-9245-4bd6-8b0e-ca3776b5260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269a3bc-6b8a-4fa9-b11a-aeda1edc4813",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5239f21-d8e0-4575-9b3d-d11679dc987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_attrbutes_to_vec_tf(attributes):\n",
    "    # Convert to tensor - transpose as tensorflow looks at batch first\n",
    "    batch_attributes_as_bool_tensor = tf.transpose(tf.stack(list(attributes.values())))\n",
    "    # Convert from boolean to float - true = 1, false = -1\n",
    "    batch_attributes_as_float_tensor = tf.where(batch_attributes_as_bool_tensor, 1.,-1.)\n",
    "    input_noise = tf.random.normal([128, 24])\n",
    "    attributes_input = tf.concat([batch_attributes_as_float_tensor, input_noise], axis=-1)\n",
    "    return attributes_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad6d895d-84f6-4ff1-aac8-0a739e82c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function #execute code in graph mode for efficiency\n",
    "def train_step(images, attributes):\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        \n",
    "        # Generate noise to pass to generator\n",
    "        attribute_input = convert_attrbutes_to_vec_tf(attributes)\n",
    "\n",
    "        # Make generated image and pass to discriminator for prediction\n",
    "        generated_images = generator(attribute_input, training=True)\n",
    "        real_output = discriminator((images, attribute_input), training=True)\n",
    "        fake_output = discriminator((generated_images, attribute_input), training=True)\n",
    "\n",
    "        # Calculate loss\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "        # Keep an eye on what discriminator is deciding (visualised in tensorboard)\n",
    "        mean_real_output = tf.math.reduce_mean(tf.nn.sigmoid(real_output))\n",
    "        mean_fake_output = tf.math.reduce_mean(tf.nn.sigmoid(fake_output))\n",
    "\n",
    "    # Calculate gradients - the derivative of the loss with respect to the model weights\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    # Tweak model weights based on optimiser calculations\n",
    "    # zip function is used to align the array of 128 (or diff batch size) gradients with the 128 trainable variables that the gradients will update\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return attribute_input, gen_loss, disc_loss, mean_real_output, mean_fake_output\n",
    "    \n",
    "def train(dataset, epochs, num_examples_to_generate):\n",
    "    \n",
    "    # tensorboard callback\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir='tensorboard_log/' + current_time\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch, attribute_batch in dataset:\n",
    "            attribute_input, gen_loss, disc_loss, mean_real_output, mean_fake_output = train_step(image_batch, attribute_batch)\n",
    "\n",
    "        # Produce images for the GIF as you go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 1, attribute_input[:num_examples_to_generate])\n",
    "        \n",
    "        # save data for tensorboard\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('gen_loss', gen_loss, step=epoch)\n",
    "            tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n",
    "            tf.summary.scalar('mean_real_output', mean_real_output, step=epoch)\n",
    "            tf.summary.scalar('mean_fake_output', mean_fake_output, step=epoch)\n",
    "\n",
    "        # Save the model every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "    \n",
    "# Generate and save images\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False. This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "    image_folder = 'generated_images/'\n",
    "    image_filepath = image_folder + 'image_at_epoch_{:04d}.png'\n",
    "    plt.savefig(image_filepath.format(epoch))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fed0f9-335b-449a-b0a7-bce76e985722",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Generator and discriminator are trained simultaneously.  \n",
    "\n",
    "It is important that the generator and discriminator do not overpower each other (i.e. they train at a similar rate)\n",
    "\n",
    "At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits. This may take about one minute / epoch with the default settings on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819499f0-4d86-4b01-aeae-94b2a78456fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_dir = checkpoint_dir + '/20210508-162827'\n",
    "#checkpoint.restore(tf.train.latest_checkpoint(restore_dir))\n",
    "\n",
    "EPOCHS = 250\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "train(ds_train, EPOCHS, num_examples_to_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8254a-327b-430a-b295-7c45ed401539",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743573b2-faad-4292-84af-a03d49aa6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837a810-d061-4f96-a78a-588910ba4826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1c59f47-8c5b-4df1-aea7-2b5b8f42daea",
   "metadata": {},
   "source": [
    "# Attribute Synonym Mapping\n",
    "\n",
    "In order to process text to generate an image, we need to search the text for attributes. \n",
    "\n",
    "The Celeb_A dataset has 40 attributes which we have mapped to additional synonyms.\n",
    "\n",
    "We have then created a function to take the text and search for attributes by first looking for trigrams and iterating down to unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c4840-2bec-40c0-8367-c1f82093a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of synonyms aligned with attributes per Celeb_A dataset\n",
    "\n",
    "dict_attribute_to_synonym = {\n",
    "    'Attractive': ['attractive', 'beautiful', 'alluring', 'glamorous', 'lovely', 'inviting', 'fair', 'enticing', \n",
    "                   'interesting', 'charming', 'pleasant', 'good-looking', 'tempting', 'gorgeous', 'pretty',\n",
    "                   'engaging', 'pleasing', 'handsome', 'adorable', 'agreeable', 'comely', 'enchanting'],\n",
    "    'Bags_Under_Eyes': ['bags under eyes', 'dark circles under eyes'],\n",
    "    'Bald': ['bald', 'hairless', 'baldheaded', 'no hair', 'little hair', 'abrupt', 'shaven', 'depilated', 'plain', \n",
    "             'direct', 'stark', 'bald headed', 'glabrous', 'austere', 'downright', 'forthright', 'outright'],\n",
    "    'Big_Lips': ['big lips', 'pouty lips', 'round lips', 'plump lips', 'puffy lips', 'full lips', \n",
    "                 'fat lips', 'big mouth', 'massive mouth', 'thick lips', 'fleshy lips', 'puffy lips', 'big-lipped'],\n",
    "    'Bangs': ['bangs', 'fringe'],\n",
    "    'Big_Nose': ['big nose', 'big-nosed', 'large nose', 'schnoz', 'huge nose'],\n",
    "    'Black_Hair': ['black hair', 'jet black hair', 'inky hair', 'pitch-black hair', 'coal-black hair', 'inky-black hair', 'jet-black hair', \n",
    "                   'raven hair'],\n",
    "    'Blond_Hair': ['blond hair', 'fair hair', 'blonde hair', 'yellow hair', 'golden hair', 'light hair', 'sandy hair', \n",
    "                   'bottle blond hair', 'honey blond hair', 'golden-yellow hair', 'platinum blond hair', \n",
    "                   'platinum hair', 'sandy blond hair'],\n",
    "    'Brown_Hair': ['brown hair', 'chestnut hair', 'dark brown hair', 'brownish hair', 'light brown hair', \n",
    "                   'caramel brown hair', 'dark hair', 'walnut brown hair', 'brown-haired hair', \n",
    "                   'chocolate brown hair', 'chocolate hair', 'chocolate coloured hair', 'cinnamon brown hair', \n",
    "                   'hazel hair', 'maple brown hair'],\n",
    "    'Bushy_Eyebrows': ['bushy eyebrows', 'thick eyebrows', 'hairy eyebrows', 'heavy eyebrows', 'intense eyebrows', \n",
    "                       'massive eyebrows'],\n",
    "    'Chubby':['chubby', 'plump', 'tubby', 'pudgy', 'rotund', 'overweight', 'chunky', 'portly', 'stout', 'fleshy', 'corpulent', \n",
    "              'fat', 'dumpy', 'obese', 'beefy', 'heavy', 'meaty', 'gross', 'bulky', 'fat man', 'full-figured', 'hefty', \n",
    "              'thick'],\n",
    "    'Double_Chin': ['double chin', 'double-chinned', 'double chinned'],\n",
    "    'Eyeglasses': ['eyeglasses', 'glasses', 'spectacles'],\n",
    "    'Goatee': ['goatee', 'anchor beard', 'balbo beard', 'facial hair'],\n",
    "    'Gray_Hair': ['gray hair', 'grey hair', 'grey haired', 'white hair', 'salt and pepper hair', 'salt-and-pepper', \n",
    "                  'grizzled', 'white haired', 'angling', 'gray haired', 'gray-haired', 'silver hair', 'greying hair', \n",
    "                  'white-haired'],\n",
    "    'Heavy_Makeup': ['heavy makeup', 'thick makeup', 'full makeup'],\n",
    "    'High_Cheekbones': ['high cheekbones', 'prominent cheekbones'],\n",
    "    'Male': ['male', 'man', 'he', 'guy','gentleman', 'dude'],\n",
    "    'Mouth_Slightly_Open': ['open mouth'],\n",
    "    'Mustache': ['mustache', 'whisker', 'mustachio'],\n",
    "    'Narrow_Eyes': ['narrow eyes', 'little eyes', 'slender eyes', 'small eyes', 'blinkered eyes', 'skinny eyes'],\n",
    "    'Oval_Face': ['oval face', 'spherical face', 'oval smile', 'round look', 'round face'],\n",
    "    'No_Beard': ['no beard', 'beardless', 'clean shaven', 'shaved'],\n",
    "    'Pale_Skin': ['pale skin', 'white skin', 'pale', 'pallor', 'blonde skin', 'fair skin', 'ghost like', 'ghostly skin', 'ghostly complextion'],\n",
    "    'Pointy_Nose': ['pointy nose', 'pointed nose', 'pointy root', 'sharp nose', 'keen nose'],\n",
    "    'Receding_Hairline': ['receding hairline', 'getting bald', 'going bald', 'little hair', 'alopecia', 'losing hair', 'balding'],\n",
    "    'Rosy_Cheeks': ['rosy cheeks', 'red cheek', 'flushed face', 'pink face', 'bright cheek'],\n",
    "    'Sideburns': ['sideburns', 'shiskers', 'side-whiskers', 'mutton chop'],\n",
    "    'Smiling': ['smiling', 'smile', 'grin', 'happy', 'grinning', 'cheerful', 'smiled'],\n",
    "    'Straight_Hair':['straight hair', 'sleek hair', 'straight hair style'],\n",
    "    'Wavy_Hair': ['wavy hair', 'curly hair', 'curls', 'curled hair'],\n",
    "    'Earrings': ['earrings', 'earing', 'studs'],\n",
    "    'Hat': ['hat', 'bonnet', 'cap', 'helmet', 'hood', 'beret'],\n",
    "    'Lipstick': ['lipstick', 'lip rouge', 'lip gloss', 'makeup', 'chapstick'],\n",
    "    'Necklace': ['necklace', 'pendant', 'choker', 'necklet', 'bead', 'pearls'],\n",
    "    'Necktie': ['necktie', 'tie', 'cravat', 'bow tie', 'black tie'],\n",
    "    'Young': ['young', 'youth', 'youthful', 'junior', 'youger', 'children']\n",
    "}\n",
    "\n",
    "# Create reversed dictionary of synonym to attribute\n",
    "dict_synonym_to_attribute = {}\n",
    "for key, value in dict_attribute_to_synonym.items():\n",
    "    for sub_value in value:\n",
    "        dict_synonym_to_attribute[sub_value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca9030a-c525-413d-82a6-4a6215e2d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_synonym_to_attribute\n",
    "dict_synonym_to_attribute['pretty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee6b0d-13c9-4afe-9e38-9fa054ec8150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process text to attributes\n",
    "\n",
    "def get_attributes_from_text(text):\n",
    "    \"\"\"\n",
    "    process text to extract attributes matched to celeb_a dataset\n",
    "    input: string\n",
    "    output: list of attributes as strings\n",
    "    \"\"\"\n",
    "    text = text.split(' ')\n",
    "    attr_list = []\n",
    "    \n",
    "    # Check for triigrams and delete from description if present\n",
    "    for i in reversed(range(len(text) - 2)):\n",
    "\n",
    "        try:\n",
    "            attr_list.append(dict_synonym_to_attribute[' '.join([text[i], text[i+1], text[i+2]])])\n",
    "            del text[i]\n",
    "            del text[i]\n",
    "            del text[i]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Check for bigrams and delete from description if present\n",
    "    for i in reversed(range(len(text) - 1)):\n",
    "\n",
    "        try:\n",
    "            attr_list.append(dict_synonym_to_attribute[' '.join([text[i], text[i+1]])])\n",
    "            del text[i]\n",
    "            del text[i]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Check for unigrams and delete from description if present\n",
    "    for i in reversed(range(len(text))):\n",
    "        try:\n",
    "            attr_list.append(dict_synonym_to_attribute[text[i]])\n",
    "            del text[i]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return attr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69384b39-7f59-46e2-8ddf-2a63658e85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'the young man with black hair entered the room'\n",
    "test = 'the pretty girl with blond hair smiled at me'\n",
    "test = 'the pretty girl with chocolate coloured hair smiled at me'\n",
    "get_attributes_from_text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f7f9b-7954-4e9f-b7a8-b2d3064bebb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d86c584-cdd8-44f9-a414-73b0c185c35a",
   "metadata": {},
   "source": [
    "# Generate images according to attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80efaec6-d7a9-4d25-a805-9b1b26f2b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087737c-c242-420f-93d5-a0413f33c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of attributes to bool value\n",
    "def create_attribute_base_dict():\n",
    "    attribute_base_dict = {}\n",
    "\n",
    "    for idx, attribute in enumerate(attributes.keys()):\n",
    "        attribute_base_dict[attribute] = False\n",
    "        \n",
    "    return attribute_base_dict\n",
    "\n",
    "attribute_base_dict = create_attribute_base_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75331ada-6a4c-4736-9cce-e41fbcf826fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_bool_ordered(attribute_base_dict, attribute_to_find):\n",
    "    \"\"\"\n",
    "    order boolean values in same order as received by model\n",
    "    output: list of boolean values\n",
    "    \"\"\"\n",
    "    attribute_to_find = list(attribute_to_find)\n",
    "    \n",
    "    attribute_values = []\n",
    "    for attribute in sorted(attribute_base_dict.keys()):\n",
    "        attribute_values.append(True if attribute in attribute_to_find else False)\n",
    "        \n",
    "    return attribute_values\n",
    "\n",
    "def attributes_multiple_list(attribute_to_find):\n",
    "    \"\"\"\n",
    "    create list of lists of attributes in ordered bool form\n",
    "    output: list of lists of boolean values\n",
    "    \"\"\"\n",
    "    if any(isinstance(item, list) for item in attribute_to_find):\n",
    "        attribute_values = []\n",
    "        for attribute_list in attribute_to_find:\n",
    "            attribute_values.append(attributes_bool_ordered(attribute_base_dict, attribute_list))\n",
    "        \n",
    "    else:\n",
    "        attribute_values = attributes_bool_ordered(attribute_base_dict, attribute_to_find)\n",
    "    return attribute_values\n",
    "\n",
    "def convert_attrbutes_to_vec(attributes):\n",
    "    \"\"\"\n",
    "    prepare attributes for image generation\n",
    "    input: list of ordered boolean values\n",
    "    output: tensor of shape (num_images, 40)\n",
    "    \"\"\"\n",
    "    # Order attributes\n",
    "    #attributes = attributes_bool_ordered(attribute_base_dict, attributes)\n",
    "    attributes = attributes_multiple_list(attributes)\n",
    "    print('attributes length', len(attributes))\n",
    "    \n",
    "    # Convert to tensor\n",
    "    #print('p1: ',attribute_embeddings.shape, type(attribute_embeddings))\n",
    "    batch_attributes_as_bool_tensor = tf.stack(list(attributes))\n",
    "    print('p2: ',batch_attributes_as_bool_tensor.shape, type(batch_attributes_as_bool_tensor))\n",
    "    if len(batch_attributes_as_bool_tensor.shape) == 1:\n",
    "        num_images = 1\n",
    "    else:\n",
    "        num_images = batch_attributes_as_bool_tensor.shape[0]\n",
    "    batch_attributes_as_bool_tensor = tf.reshape(batch_attributes_as_bool_tensor, [num_images,40])\n",
    "    print('p2: ',batch_attributes_as_bool_tensor.shape, type(batch_attributes_as_bool_tensor))\n",
    "    \n",
    "    # Convert from boolean to float\n",
    "    batch_attributes_as_float_tensor = tf.where(batch_attributes_as_bool_tensor, 1.,-1.)\n",
    "    print('p3', batch_attributes_as_float_tensor.shape, type(batch_attributes_as_float_tensor))\n",
    "    \n",
    "    # Create projection of attributes into embedding space\n",
    "    #attribute_input = batch_attributes_as_float_tensor\n",
    "    \n",
    "    input_noise = tf.random.normal([num_images, 24])\n",
    "    #input_noise = tf.zeros([num_images, 24])\n",
    "    attributes_input = tf.concat([batch_attributes_as_float_tensor, input_noise], axis=-1)\n",
    "    return attributes_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b1112-4db8-4b9f-987d-869fe7162485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa472770-be9e-4536-95a1-425034a90639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attribute_to_test = ['Eyeglasses', 'Male', 'Attractive', 'Blond_Hair']\n",
    "attribute_to_test = [['Eyeglasses', 'Male', 'Attractive'], ['Eyeglasses', 'Male', 'Attractive', 'Blond_Hair'], ['Eyeglasses', 'Male', 'Blond_Hair'], \n",
    "                     ['Attractive', 'Blond_Hair'],['Attractive', 'Blond_Hair', 'Wearing_Lipstick']]\n",
    "\n",
    "attribute_to_test = [['Eyeglasses', 'Male', 'Attractive'], ['Eyeglasses', 'Male', 'Attractive'], ['Eyeglasses', 'Male', 'Attractive'],\n",
    "                     ['Eyeglasses', 'Male', 'Attractive'], ['Eyeglasses', 'Male', 'Attractive']]\n",
    "\n",
    "#attribute_to_test = [['Eyeglasses', 'Attractive'], ['Eyeglasses', 'Attractive', 'Blond_Hair'], ['Eyeglasses', 'Blond_Hair'], \n",
    " #                    ['Attractive', 'Blond_Hair'],['Attractive', 'Blond_Hair', 'Wearing_Lipstick']]\n",
    "\n",
    "#attribute_to_test = [['Attractive'], ['Attractive', 'Blond_Hair'], ['Blond_Hair'], \n",
    " #                    ['Attractive', 'Brown_Hair'],['Attractive', 'Blond_Hair', 'Wearing_Lipstick']]\n",
    "\n",
    "\n",
    "attributes_test = convert_attrbutes_to_vec(attribute_to_test)\n",
    "print(attributes_test.shape, type(attributes_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7736a3-9258-43cd-b935-eb728e286b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for text to attributes\n",
    "def convert_text_to_vector(text):\n",
    "    attributes_list = get_attributes_from_text(text)\n",
    "    attributes = convert_attrbutes_to_vec(attributes_list)\n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977a525-de0e-4702-915b-52eed21d67be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f77e2e-ad04-460b-89c3-0ef41d8beca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating an image based on specific attributes\n",
    "\n",
    "attributes_test = convert_attrbutes_to_vec(attribute_to_test)\n",
    "generated_image = generator(attributes_test, training=False)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "num_images = generated_image.shape[0]\n",
    "\n",
    "for i in range(num_images):\n",
    "    cols = 5\n",
    "    rows = int(np.ceil(num_images / cols))\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(generated_image[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3177d1c-3000-4b5b-8944-fccbbfac1a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'the young man with black hair entered the room'\n",
    "test = 'the pretty girl with blond hair smiled at me'\n",
    "#test = 'the pretty girl with chocolate coloured hair smiled at me'\n",
    "test = 'the bald girl smiled at me'\n",
    "test = convert_text_to_vector(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d131c-a481-4dcf-a590-507cf4628ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating an image based on text\n",
    "\n",
    "generated_image = generator(test, training=False)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "num_images = generated_image.shape[0]\n",
    "\n",
    "for i in range(num_images):\n",
    "    cols = 5\n",
    "    rows = int(np.ceil(num_images / cols))\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(generated_image[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adda10c-4043-435b-87ed-8f213867a8fb",
   "metadata": {},
   "source": [
    "# Distribution of Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bdb0c8-2049-416c-8389-cfa5ec73e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_distribution = []\n",
    "\n",
    "for images, attributes in ds_train:\n",
    "    attributes_distribution.append(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ec1a66-053f-44f5-b5ad-b8c76fcab12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = attributes.keys())\n",
    "\n",
    "for batch in attributes_distribution:\n",
    "    df_temp = pd.DataFrame(batch)\n",
    "    df = df.append(df_temp)\n",
    "    \n",
    "print(len(df))\n",
    "\n",
    "#df = df * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d203a56-d681-4a99-b669-16ebd1c2c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = pd.DataFrame(df.sum()).reset_index()\n",
    "df_count.columns = ['Attribute', 'Count']\n",
    "#df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9df17-e7a9-473d-bdfd-2bf7f3ca3874",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "sns.barplot(x='Attribute', y='Count', data=df_count)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd48a27-228b-4af1-9e95-049243ad4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "    return PIL.Image.open('./generated_images/image_at_epoch_{:04d}.png'.format(epoch_no))\n",
    "\n",
    "display_image(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6f73f-2f37-4640-bd83-7be7097a9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_random = np.random.choice(a=[False, True], size=(128,40))\n",
    "print(attributes_random[1].shape)\n",
    "attributes_random[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc4956-d857-4926-9ec2-303f6542a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4d3b4-fe28-46ab-adef-ba2a24173c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1aa71e-83d4-4f3b-9fb1-76271d59c5ac",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Dataset - https://www.tensorflow.org/datasets/catalog/celeb_a\n",
    "- Model design - https://www.tensorflow.org/tutorials/generative/dcgan\n",
    "- Model design - https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-a-cifar-10-small-object-photographs-from-scratch/\n",
    "- Model design - Conditional Gan - https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/\n",
    "- Model design - https://machinelearningmastery.com/how-to-train-a-progressive-growing-gan-in-keras-for-synthesizing-faces/\n",
    "- Gan Hacks - Improving Gan Performance - https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/\n",
    "- Adding noise - https://machinelearningmastery.com/how-to-improve-deep-learning-model-robustness-by-adding-noise/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6453c788-c73e-4cb6-9696-5d8fd4ecab4c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e51f41-fa8a-478f-a7d5-b1cc6c091457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
