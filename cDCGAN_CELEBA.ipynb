{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8486eb71-b2fc-4249-a9d1-3eb1988bc1d2",
   "metadata": {},
   "source": [
    "### Imports and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84797fad-600c-492c-a402-167a3194654a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version:  2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('tensorflow version: ', tf.__version__)\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b85270-f1fd-43b0-a583-c3a9e0bddcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n",
      "GPUs:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e775c-67ff-45eb-9874-7ce1de673cc4",
   "metadata": {},
   "source": [
    "### Ways to iterate over the dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3bd65e10-383c-4211-8d54-b8c70803dcdc",
   "metadata": {},
   "source": [
    "# Dictionary\n",
    "\n",
    "ds = ds.take(1)  # Only take a single example\n",
    "\n",
    "for example in ds:  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "    print(list(example.keys()))\n",
    "    image = example[\"image\"]\n",
    "    label = example[\"attributes\"]\n",
    "    print(image.shape, label)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "289fac94-6260-4492-a861-d91ad1eac75d",
   "metadata": {},
   "source": [
    "# Show examples\n",
    "\n",
    "ds, info = tfds.load('celeb_a', split='train', shuffle_files=True, with_info=True)\n",
    "fig = tfds.show_examples(ds, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f04941-4bfa-4ee6-8058-66d03e6257a2",
   "metadata": {},
   "source": [
    "### Load and Prepare the Dataset / Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8fdcde6-47b8-4f03-ac2b-90d157409563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.tensorflow.org/datasets/keras_example\n",
    "\n",
    "(ds_train_raw, ds_test_raw), ds_info = tfds.load(\n",
    "    'celeb_a',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "def preprocess(data):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    image = data['image']\n",
    "    label = data['attributes']\n",
    "    image = tf.image.resize(image, [28, 28])\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    return image, label\n",
    "\n",
    "ds_train = ds_train_raw.map(\n",
    "    preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd49434-8f2a-441e-8852-34c0c883527e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 28, 28, 3), {5_o_Clock_Shadow: (None,), Arched_Eyebrows: (None,), Attractive: (None,), Bags_Under_Eyes: (None,), Bald: (None,), Bangs: (None,), Big_Lips: (None,), Big_Nose: (None,), Black_Hair: (None,), Blond_Hair: (None,), Blurry: (None,), Brown_Hair: (None,), Bushy_Eyebrows: (None,), Chubby: (None,), Double_Chin: (None,), Eyeglasses: (None,), Goatee: (None,), Gray_Hair: (None,), Heavy_Makeup: (None,), High_Cheekbones: (None,), Male: (None,), Mouth_Slightly_Open: (None,), Mustache: (None,), Narrow_Eyes: (None,), No_Beard: (None,), Oval_Face: (None,), Pale_Skin: (None,), Pointy_Nose: (None,), Receding_Hairline: (None,), Rosy_Cheeks: (None,), Sideburns: (None,), Smiling: (None,), Straight_Hair: (None,), Wavy_Hair: (None,), Wearing_Earrings: (None,), Wearing_Hat: (None,), Wearing_Lipstick: (None,), Wearing_Necklace: (None,), Wearing_Necktie: (None,), Young: (None,)}), types: (tf.float32, {5_o_Clock_Shadow: tf.bool, Arched_Eyebrows: tf.bool, Attractive: tf.bool, Bags_Under_Eyes: tf.bool, Bald: tf.bool, Bangs: tf.bool, Big_Lips: tf.bool, Big_Nose: tf.bool, Black_Hair: tf.bool, Blond_Hair: tf.bool, Blurry: tf.bool, Brown_Hair: tf.bool, Bushy_Eyebrows: tf.bool, Chubby: tf.bool, Double_Chin: tf.bool, Eyeglasses: tf.bool, Goatee: tf.bool, Gray_Hair: tf.bool, Heavy_Makeup: tf.bool, High_Cheekbones: tf.bool, Male: tf.bool, Mouth_Slightly_Open: tf.bool, Mustache: tf.bool, Narrow_Eyes: tf.bool, No_Beard: tf.bool, Oval_Face: tf.bool, Pale_Skin: tf.bool, Pointy_Nose: tf.bool, Receding_Hairline: tf.bool, Rosy_Cheeks: tf.bool, Sideburns: tf.bool, Smiling: tf.bool, Straight_Hair: tf.bool, Wavy_Hair: tf.bool, Wearing_Earrings: tf.bool, Wearing_Hat: tf.bool, Wearing_Lipstick: tf.bool, Wearing_Necklace: tf.bool, Wearing_Necktie: tf.bool, Young: tf.bool})>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a0b1f7b-390f-442c-bb1d-0aebb54ab5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 28, 28, 3)\n",
      "dict_keys(['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young'])\n"
     ]
    }
   ],
   "source": [
    "for images, attributes in ds_train:\n",
    "    break\n",
    "print(images.shape)\n",
    "print(attributes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a48db690-6b15-4a92-a404-bc7f6abc97c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([28, 28, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10da6ec8-657a-4eaa-9375-133a11c17d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.numpy().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48fa7f2-d400-4205-a90f-8a4df7447bdd",
   "metadata": {},
   "source": [
    "## Create the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ab8cd-ac07-42bb-8aa6-a43131505c28",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "The generator uses tf.keras.layers.Conv2DTranspose (upsampling) layers to produce an image from a seed (random noise). Start with a Dense layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the tf.keras.layers.LeakyReLU activation for each layer, except the output layer which uses tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c094dd01-6926-4690-95bf-0d1947c44ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attribute vectors\n",
    "#attributes.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86fad644-910e-40c3-a27b-3118bbf4229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 100), dtype=tf.float32, name=None), name='embedding/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 49), dtype=tf.float32, name=None), name='dense/BiasAdd:0', description=\"created by layer 'dense'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 7, 7, 1), dtype=tf.float32, name=None), name='reshape/Reshape:0', description=\"created by layer 'reshape'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 100), dtype=tf.float32, name=None), name='multiply/mul:0', description=\"created by layer 'multiply'\")\n",
      "(None, 100)\n",
      "(None, 1, 100)\n"
     ]
    }
   ],
   "source": [
    "label = layers.Input(shape=(1,))\n",
    "#label_embedding = layers.Flatten()(layers.Embedding(40, 100)(label))\n",
    "label_embedding = layers.Embedding(40, 100)(label)\n",
    "print(label_embedding)\n",
    "n_nodes = 7 * 7\n",
    "li = layers.Dense(n_nodes)(label_embedding)\n",
    "print(li)\n",
    "li = layers.Reshape((7,7,1))(li)\n",
    "print(li)\n",
    "noise = layers.Input(shape=(100,))\n",
    "model_input = layers.multiply([noise, label_embedding])\n",
    "print(model_input)\n",
    "print(noise.shape)\n",
    "print(label_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6bf399f-2b44-4f53-bac1-50aec5d5e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attributes.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "286b805e-96f4-4225-baa7-0d9eb9810c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1:  (40, 100)\n",
      "(128, 100)\n"
     ]
    }
   ],
   "source": [
    "# Create random values to be multiple with random attributes to form noise for generator input\n",
    "noise_dim = 100\n",
    "attribute_embeddings = tf.Variable(initial_value=tf.random.normal([40, noise_dim]), trainable=True) # Do this once only, but add to optimizers\n",
    "print('p1: ',attribute_embeddings.shape)\n",
    "\n",
    "def initiate_seed_weights(attributes, attribute_embeddings):\n",
    "    # Convert to tensor\n",
    "    #print(attributes.values().shape)\n",
    "    #print('p2: ',attribute_embeddings.shape)\n",
    "    batch_attributes_as_bool_tensor = tf.transpose(tf.stack(list(attributes.values())))\n",
    "    #print(batch_attributes_as_bool_tensor.shape)\n",
    "    # Convert from boolean to float\n",
    "    batch_attributes_as_float_tensor = tf.where(batch_attributes_as_bool_tensor, 1.,-1.)\n",
    "    #print(batch_attributes_as_float_tensor.shape)\n",
    "    # Create projection of attributes into embedding space\n",
    "    attribute_input = tf.matmul(batch_attributes_as_float_tensor, attribute_embeddings) \n",
    "    return attribute_input\n",
    "\n",
    "attributes_random = np.random.choice(a=[False, True], size=(128,40)) # this doesn't seem to be used anywhere\n",
    "attributes_noise = initiate_seed_weights(attributes, attribute_embeddings)\n",
    "print(attributes_noise.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc864d6c-5825-431d-b8b8-5d0f9efbed55",
   "metadata": {},
   "source": [
    "# Old Code\n",
    "noise_dim = 100\n",
    "attribute_embeddings = tf.Variable(initial_value=tf.random.normal([40, noise_dim]), trainable=True)\n",
    "print(attribute_embeddings.shape)\n",
    "\n",
    "attributes_random = np.random.choice(a=[False, True], size=(128,40))\n",
    "# convert from bool\n",
    "batch_attributes_as_bool_tensor = tf.transpose(tf.stack(list(attributes.values())))\n",
    "attributes_random = tf.where(batch_attributes_as_bool_tensor, 1.,-1.)\n",
    "print(attributes_random.shape)\n",
    "\n",
    "# Create noise vector\n",
    "attributes_noise = tf.matmul(attributes_random, attribute_embeddings)\n",
    "print(attributes_noise.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af496087-0c1a-4cec-bc88-2d1608f1faa7",
   "metadata": {},
   "source": [
    "batch_attributes_as_bool_tensor = tf.transpose(tf.stack(list(attributes.values())))\n",
    "print(batch_attributes_as_bool_tensor.shape)\n",
    "batch_attributes_as_float_tensor = tf.where(batch_attributes_as_bool_tensor, 1.,-1.)\n",
    "print(batch_attributes_as_float_tensor.shape)\n",
    "print(tf.matmul(batch_attributes_as_float_tensor, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c81effbe-eff2-479e-bbc5-a4db0c90618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator\n",
    "def make_generator_model():\n",
    "    \n",
    "    model = tf.keras.Sequential(name='Generator')\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Conv2D(3, (3,3), activation='sigmoid', padding = 'same', use_bias=False))\n",
    "    assert model.output_shape == (None, 28, 28, 3)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9a17a57-5c64-48dc-b3ea-2a44f3256d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(1, 28, 28, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an image using the untrained generator\n",
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "print(generated_image.dtype)\n",
    "print(generated_image.shape)\n",
    "\n",
    "#plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "plt.imshow(generated_image[0])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e9d8dff-d629-4a81-8f12-9119852d4272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(128, 28, 28, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "generated_image = generator(attributes_noise, training=False)\n",
    "print(generated_image.dtype)\n",
    "print(generated_image.shape)\n",
    "\n",
    "plt.imshow(generated_image[0])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(generated_image[1])\n",
    "plt.show()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a7e409-cdd3-436b-b6a6-d7a8dbf41783",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "The discriminator is a CNN-based image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088124b-cd44-4a5a-b39d-5db022e36515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59c79c7e-d9f1-4294-877a-456cd79486ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([28, 28, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = tf.random.normal([28, 28, 3])\n",
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d73e3f30-8cf1-4c3c-8fca-cf5fe1a03540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    \n",
    "    # label input\n",
    "    input_label = layers.Input(shape=(100))\n",
    "    input_image = layers.Input(shape=(28,28,3))\n",
    "    #noise = tf.random.normal([28, 28, 3])\n",
    "\n",
    "    #multi_image_model = tf.keras.Sequential(name='Discriminator_multi_image')\n",
    "    #multi_image_model.add(layers.Conv3D(64, (5, 5, 5), padding='same'))\n",
    "    \n",
    "    \n",
    "    conv_model = tf.keras.Sequential(name='Discriminator_conv_model')\n",
    "    # downsample\n",
    "    conv_model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n",
    "    conv_model.add(layers.LeakyReLU())\n",
    "    conv_model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #downsample\n",
    "    conv_model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    conv_model.add(layers.BatchNormalization())\n",
    "    conv_model.add(layers.LeakyReLU())\n",
    "    conv_model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # classifier\n",
    "    conv_model.add(layers.Flatten())\n",
    "    \n",
    "    # Concatentate the new one here\n",
    "    # The feed the concatenated model into the dense layer\n",
    "    \n",
    "    #multi_image_feature = multi_image_model(tf.expand_dims(input_image, axis=0))\n",
    "    #multi_image_feature = tf.squeeze(multi_image_feature, axis=0)\n",
    "    \n",
    "    img_feature_vec = conv_model(input_image) #input_image+noise*10)\n",
    "    \n",
    "    # concat label as a channel\n",
    "    merge = layers.Concatenate()([img_feature_vec, input_label])\n",
    "    \n",
    "    # multi-layer perceptrons\n",
    "    mlp = tf.keras.Sequential(name='Discriminator_mlp')\n",
    "    mlp.add(layers.Dense(128))\n",
    "    mlp.add(layers.LeakyReLU())\n",
    "    mlp.add(layers.Dropout(0.3))\n",
    "    mlp.add(layers.Dense(1))\n",
    "    \n",
    "    output = mlp(merge)\n",
    "    \n",
    "    # Need to create new model here with correct shapes which need to be defined as inputs\n",
    "    model = tf.keras.Model([input_image, input_label], output, name='Discriminator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89f74ea2-8b35-49f2-ae72-2fd1f4002425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the generated images as real or fake using the discriminator.\n",
    "# Model is trained to output positive values for real images and negative values for fake images\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "#decision = discriminator((generated_image, attributes_noise))\n",
    "#print(decision.shape)\n",
    "#print(decision[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c572e1-6fe8-4b65-9ed0-caddc810f2fb",
   "metadata": {},
   "source": [
    "## Define the loss and optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "777205f3-1f15-48fe-a983-d902fe085cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aaea0a-313c-4cdb-be79-6160dfd8723b",
   "metadata": {},
   "source": [
    "### Discriminator loss\n",
    "\n",
    "Quantifies how well the discriminator is able to distinguish real images from fakes.  \n",
    "\n",
    "It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "692afa5b-8b3e-4b47-91ef-ed50b6a408bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b1be0-18b8-4cb2-bf2d-0049a4e996ed",
   "metadata": {},
   "source": [
    "### Generator loss\n",
    "\n",
    "Quantifies how well it was able to trick the discriminator.  \n",
    "\n",
    "If the generator is performng well, the discriminator will classify the fake images as real (or 1). This function compares the discriminators decisions on the generated images to an array of 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b9ac231-f711-436d-95fe-2ada50c54162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf04dc-1cc1-4eee-9fe6-7ebe497ddbd3",
   "metadata": {},
   "source": [
    "### Optimisers\n",
    "\n",
    "Two different optimisers required since the two networks are trained separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d05ee072-92c9-4553-b0f8-80014a9d4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "#generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "#discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "attribute_embeddings = tf.Variable(initial_value=tf.random.normal([40, noise_dim]), trainable=True) # Do this once only, but add to optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed13888-a451-4fd2-8dba-3e9e05824c48",
   "metadata": {},
   "source": [
    "### Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cfaf80e-9245-4bd6-8b0e-ca3776b5260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator,\n",
    "                                 attribute_embeddings=attribute_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269a3bc-6b8a-4fa9-b11a-aeda1edc4813",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5239f21-d8e0-4575-9b3d-d11679dc987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_seed_weights(attributes, attribute_embeddings):\n",
    "    # Convert to tensor\n",
    "    #print(attributes.values().shape)\n",
    "    #print('p2: ',attribute_embeddings.shape)\n",
    "    batch_attributes_as_bool_tensor = tf.transpose(tf.stack(list(attributes.values())))\n",
    "    #print(batch_attributes_as_bool_tensor.shape)\n",
    "    # Convert from boolean to float\n",
    "    batch_attributes_as_float_tensor = tf.where(batch_attributes_as_bool_tensor, 1.,-1.)\n",
    "    #print(batch_attributes_as_float_tensor.shape)\n",
    "    # Create projection of attributes into embedding space\n",
    "    attribute_input = tf.matmul(batch_attributes_as_float_tensor, attribute_embeddings) \n",
    "    return attribute_input\n",
    "\n",
    "attributes_random = np.random.choice(a=[False, True], size=(128,40)) # this doesn't seem to be used anywhere\n",
    "attributes_noise = initiate_seed_weights(attributes, attribute_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad6d895d-84f6-4ff1-aac8-0a739e82c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, attributes):\n",
    "    #noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        \n",
    "        attribute_input = initiate_seed_weights(attributes, attribute_embeddings)\n",
    "\n",
    "        generated_images = generator(attribute_input, training=True)\n",
    "        real_output = discriminator((images, attribute_input), training=True)\n",
    "        fake_output = discriminator((generated_images, attribute_input), training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)# + [attribute_embeddings])\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables + [attribute_embeddings])\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))# + [attribute_embeddings]))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables + [attribute_embeddings]))\n",
    "    \n",
    "    return attribute_input\n",
    "    \n",
    "def train(dataset, epochs, num_examples_to_generate):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch, attribute_batch in dataset:\n",
    "            attribute_input = train_step(image_batch, attribute_batch)\n",
    "\n",
    "        # Produce images for the GIF as you go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 1, attribute_input[:num_examples_to_generate])\n",
    "\n",
    "        # Save the model every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "    \n",
    "# Generate and save images\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False. This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        #plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.imshow(predictions[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "    image_folder = 'generated_images/'\n",
    "    image_filepath = image_folder + 'image_at_epoch_{:04d}.png'\n",
    "    plt.savefig(image_filepath.format(epoch))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fed0f9-335b-449a-b0a7-bce76e985722",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Generator and discriminator are trained simultaneously.  \n",
    "\n",
    "It is important that the generator and discriminator do not overpower each other (i.e. they train at a similar rate)\n",
    "\n",
    "At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits. This may take about one minute / epoch with the default settings on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819499f0-4d86-4b01-aeae-94b2a78456fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "train(ds_train, EPOCHS, num_examples_to_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8254a-327b-430a-b295-7c45ed401539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743573b2-faad-4292-84af-a03d49aa6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440d76f-7ae1-4249-af4a-1e4cdc58dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.full((40,),-1)\n",
    "test_array[9] = 1 # blonde\n",
    "test_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4bb51-49cd-4d55-8e09-b5e2536484a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try generating an image based on specific attributes\n",
    "attributes_noise = initiate_seed_weights(attributes, attribute_embeddings)\n",
    "\n",
    "generated_image = generator(attributes_noise, training=False)\n",
    "print(generated_image.dtype)\n",
    "print(generated_image.shape)\n",
    "\n",
    "plt.imshow(generated_image[1])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd48a27-228b-4af1-9e95-049243ad4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "    return PIL.Image.open('./generated_images/image_at_epoch_{:04d}.png'.format(epoch_no))\n",
    "\n",
    "display_image(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6f73f-2f37-4640-bd83-7be7097a9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_random = np.random.choice(a=[False, True], size=(128,40))\n",
    "attributes_random[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc4956-d857-4926-9ec2-303f6542a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a3e13-855e-41f0-b7e8-1bfd149b7369",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = generator_optimizer.get_weights()\n",
    "print(len(weights))\n",
    "weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4d3b4-fe28-46ab-adef-ba2a24173c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88071b1-f0b5-4303-abc7-2b0eac828d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
