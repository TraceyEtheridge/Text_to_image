{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8486eb71-b2fc-4249-a9d1-3eb1988bc1d2",
   "metadata": {},
   "source": [
    "### Imports and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84797fad-600c-492c-a402-167a3194654a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version:  2.4.1\n",
      "tfds version:  4.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('tensorflow version: ', tf.__version__)\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "print('tfds version: ', tfds.__version__)\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime as datetime\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b85270-f1fd-43b0-a583-c3a9e0bddcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n",
      "GPUs:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e775c-67ff-45eb-9874-7ce1de673cc4",
   "metadata": {},
   "source": [
    "### Ways to iterate over the dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3bd65e10-383c-4211-8d54-b8c70803dcdc",
   "metadata": {},
   "source": [
    "# Dictionary\n",
    "\n",
    "ds = ds.take(1)  # Only take a single example\n",
    "\n",
    "for example in ds:  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "    print(list(example.keys()))\n",
    "    image = example[\"image\"]\n",
    "    label = example[\"attributes\"]\n",
    "    print(image.shape, label)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "289fac94-6260-4492-a861-d91ad1eac75d",
   "metadata": {},
   "source": [
    "# Show examples\n",
    "\n",
    "ds, info = tfds.load('celeb_a', split='train', shuffle_files=True, with_info=True)\n",
    "fig = tfds.show_examples(ds, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f04941-4bfa-4ee6-8058-66d03e6257a2",
   "metadata": {},
   "source": [
    "### Load and Prepare the Dataset / Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ab004-87d9-46dc-94e1-6b98c25b53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train_raw, ds_test_raw), ds_info = tfds.load(\n",
    "    'celeb_a',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "def preprocess(data):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    image = data['image']\n",
    "    label = data['attributes']\n",
    "    NUM_BOXES = 1\n",
    "    boxes = [[.1,.1,.9,.9,]]\n",
    "    box_indices = tf.zeros(shape=(NUM_BOXES,), dtype=tf.int32)\n",
    "    #image = tf.image.resize(image[20:-20], [64, 64], antialias=True)\n",
    "    image = tf.image.crop_and_resize(tf.expand_dims(image, axis=0), crop_size=[64, 64], boxes=boxes, box_indices=box_indices)\n",
    "    image = tf.cast(image, tf.float32) / 255. # normalise images to between 0 and 1\n",
    "    return image[0], label\n",
    "\n",
    "ds_train = ds_train_raw.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d3f71-1878-48c7-924b-1cb70ff4a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in ds_train:\n",
    "    for i in range(10):\n",
    "        plt.imshow(data[0][i])\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd49434-8f2a-441e-8852-34c0c883527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.numpy().max())\n",
    "print(images.numpy().min())\n",
    "\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b1f7b-390f-442c-bb1d-0aebb54ab5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, attributes in ds_train:\n",
    "    break\n",
    "print(images.shape)\n",
    "print(images[0].shape)\n",
    "print(attributes.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48fa7f2-d400-4205-a90f-8a4df7447bdd",
   "metadata": {},
   "source": [
    "## Create the models\n",
    "\n",
    "Adapated from https://www.tensorflow.org/tutorials/generative/dcgan which uses MNIST to create a non-conditional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ab8cd-ac07-42bb-8aa6-a43131505c28",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "The generator uses tf.keras.layers.Conv2DTranspose (upsampling) layers to produce an image from a seed (random noise). Start with a Dense layer that takes this seed as input, then upsample several times until you reach the desired image size of 64x64x1. Notice the tf.keras.layers.LeakyReLU activation for each layer, except the output layer which uses tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81effbe-eff2-479e-bbc5-a4db0c90618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator\n",
    "def make_generator_model():\n",
    "    \n",
    "    model = tf.keras.Sequential(name='Generator')\n",
    "    # add noise layer\n",
    "    model.add(layers.GaussianNoise(stddev=0.1, input_shape=(40,)))\n",
    "    # Densly connection NN layer of 8*8*256 (16,384) neurons. This is taking in the input initialisation and multiplying with the weights\n",
    "    model.add(layers.Dense(8*8*256, use_bias=False)) \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((8, 8, 256)))\n",
    "    assert model.output_shape == (None, 8, 8, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 8, 8, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 16, 16, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(32, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 32, 32, 32)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(16, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    # Output layer\n",
    "    # Activation of sigmoid is used to squash values between 0 and 1\n",
    "    model.add(layers.Conv2D(3, (3,3), activation='sigmoid', padding = 'same', use_bias=False))\n",
    "    assert model.output_shape == (None, 64, 64, 3)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f9ce0e8-353e-481b-b83b-390499839798",
   "metadata": {},
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "generated_image = generator(attributes_noise, training=False)\n",
    "print(generated_image.dtype)\n",
    "print(generated_image.shape)\n",
    "\n",
    "plt.imshow(generated_image[0])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(generated_image[1])\n",
    "plt.show()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a7e409-cdd3-436b-b6a6-d7a8dbf41783",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "The discriminator is a CNN-based image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088124b-cd44-4a5a-b39d-5db022e36515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c79c7e-d9f1-4294-877a-456cd79486ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noise = tf.random.normal([64, 64, 3])\n",
    "#noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e3f30-8cf1-4c3c-8fca-cf5fe1a03540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    \n",
    "    # label input\n",
    "    attribute_vector = layers.Input(shape=(40))\n",
    "    input_image = layers.Input(shape=(64,64,3))\n",
    "    #noise = tf.random.normal([64, 64, 3])\n",
    "    \n",
    "    conv_model = tf.keras.Sequential(name='Discriminator_conv_model')\n",
    "    \n",
    "    # gan hacks - add noise\n",
    "    conv_model.add(layers.GaussianNoise(stddev=0.1))\n",
    "    \n",
    "    # downsample - increase filters as it progresses through the model\n",
    "    conv_model.add(layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same'))\n",
    "    conv_model.add(layers.BatchNormalization())\n",
    "    conv_model.add(layers.LeakyReLU())\n",
    "    \n",
    "    #downsample\n",
    "    conv_model.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same'))\n",
    "    conv_model.add(layers.BatchNormalization())\n",
    "    conv_model.add(layers.LeakyReLU())\n",
    "    \n",
    "    #downsample\n",
    "    conv_model.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same'))\n",
    "    conv_model.add(layers.LeakyReLU())\n",
    "    conv_model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # classifier\n",
    "    conv_model.add(layers.Flatten()) # flatten to 1D array\n",
    "    \n",
    "    # Concatentate the new one here\n",
    "    # The feed the concatenated model into the dense layer\n",
    "    \n",
    "    #img_feature_vec = conv_model(input_image+noise*1)\n",
    "    img_feature_vec = conv_model(input_image)\n",
    "    \n",
    "    # concatenate layers - the image vector + the attribute labels\n",
    "    merge = layers.Concatenate()([img_feature_vec, attribute_vector])\n",
    "    \n",
    "    # Gan Hacks - deeper model\n",
    "    conv_deep_model = tf.keras.Sequential(name='Discriminator_deep')\n",
    "    conv_deep_model.add(layers.Dense(128))\n",
    "    #conv_deep_model.add(layers.Dense(256))\n",
    "    conv_deep_model.add(layers.LeakyReLU())\n",
    "    conv_deep_model.add(layers.Dropout(0.3))\n",
    "    conv_deep_model.add(layers.Dense(1))\n",
    "\n",
    "    # linear layer that takes in merge and makes a prediction based on that\n",
    "    #output = layers.Dense(1)(merge)\n",
    "    output = conv_deep_model(merge)\n",
    "    \n",
    "    # Need to create new model here with correct shapes which need to be defined as inputs\n",
    "    model = tf.keras.Model(inputs=[input_image, attribute_vector], outputs=output, name='Discriminator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f74ea2-8b35-49f2-ae72-2fd1f4002425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Models\n",
    "# Classify the generated images as real or fake using the discriminator.\n",
    "# Model is trained to output positive values for real images and negative values for fake images\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "#decision = discriminator((generated_image, attributes_noise))\n",
    "#print(decision.shape)\n",
    "#print(decision[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c572e1-6fe8-4b65-9ed0-caddc810f2fb",
   "metadata": {},
   "source": [
    "## Define the loss and optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777205f3-1f15-48fe-a983-d902fe085cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "# The discriminator has binary input (fake or real data) so a binary loss function is used\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aaea0a-313c-4cdb-be79-6160dfd8723b",
   "metadata": {},
   "source": [
    "### Discriminator loss\n",
    "\n",
    "Quantifies how well the discriminator is able to distinguish real images from fakes.  \n",
    "\n",
    "It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692afa5b-8b3e-4b47-91ef-ed50b6a408bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b1be0-18b8-4cb2-bf2d-0049a4e996ed",
   "metadata": {},
   "source": [
    "### Generator loss\n",
    "\n",
    "Quantifies how well it was able to trick the discriminator.  \n",
    "\n",
    "If the generator is performng well, the discriminator will classify the fake images as real (or 1). This function compares the discriminators decisions on the generated images to an array of 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ac231-f711-436d-95fe-2ada50c54162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf04dc-1cc1-4eee-9fe6-7ebe497ddbd3",
   "metadata": {},
   "source": [
    "### Optimisers\n",
    "\n",
    "Two different optimisers required since the two networks are trained separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ee072-92c9-4553-b0f8-80014a9d4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 40\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "attribute_embeddings = tf.Variable(initial_value=tf.random.normal([40, noise_dim]), trainable=True) # Do this once only, but add to optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed13888-a451-4fd2-8dba-3e9e05824c48",
   "metadata": {},
   "source": [
    "### Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfaf80e-9245-4bd6-8b0e-ca3776b5260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "                                 #attribute_embeddings=attribute_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269a3bc-6b8a-4fa9-b11a-aeda1edc4813",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5239f21-d8e0-4575-9b3d-d11679dc987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_seed_weights(attributes):\n",
    "    # Convert to tensor\n",
    "    #print(attributes.values().shape)\n",
    "    #print('p1: ',attribute_embeddings.shape, type(attribute_embeddings))\n",
    "    batch_attributes_as_bool_tensor = tf.transpose(tf.stack(list(attributes.values())))\n",
    "    #print('p2: ',batch_attributes_as_bool_tensor.shape, type(batch_attributes_as_bool_tensor))\n",
    "    # Convert from boolean to float\n",
    "    batch_attributes_as_float_tensor = tf.where(batch_attributes_as_bool_tensor, 1.,-1.)\n",
    "    #print('p3: ',batch_attributes_as_float_tensor.shape, type(batch_attributes_as_float_tensor))\n",
    "    # Create projection of attributes into embedding space\n",
    "    #attribute_input = tf.matmul(batch_attributes_as_float_tensor, attribute_embeddings) \n",
    "    attribute_input = batch_attributes_as_float_tensor\n",
    "    return attribute_input\n",
    "\n",
    "#attributes_random = np.random.choice(a=[False, True], size=(128,40)) # this doesn't seem to be used anywhere\n",
    "#attributes_noise = initiate_seed_weights(attributes, attribute_embeddings)\n",
    "#attributes_noise = initiate_seed_weights(attributes)\n",
    "\n",
    "#print(attributes_noise.shape)\n",
    "#print(type(attributes_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d895d-84f6-4ff1-aac8-0a739e82c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function #execute code in graph mode for efficiency\n",
    "def train_step(images, attributes):\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        \n",
    "        # Generate noise to pass to generator\n",
    "        attribute_input = initiate_seed_weights(attributes)\n",
    "\n",
    "        # Make generated image and pass to discriminator for prediction\n",
    "        generated_images = generator(attribute_input, training=True)\n",
    "        real_output = discriminator((images, attribute_input), training=True)\n",
    "        fake_output = discriminator((generated_images, attribute_input), training=True)\n",
    "\n",
    "        # Calculate loss\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "        # Keep an eye on what discriminator is deciding (visualised in tensorboard)\n",
    "        mean_real_output = tf.math.reduce_mean(tf.nn.sigmoid(real_output))\n",
    "        mean_fake_output = tf.math.reduce_mean(tf.nn.sigmoid(fake_output))\n",
    "\n",
    "    # Calculate gradients - the derivative of the loss with respect to the model weights\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    # Tweak model weights based on optimiser calculations\n",
    "    # zip function is used to align the array of 128 (or diff batch size) gradients with the 128 trainable variables that the gradients will update\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return attribute_input, gen_loss, disc_loss, mean_real_output, mean_fake_output\n",
    "    \n",
    "def train(dataset, epochs, num_examples_to_generate):\n",
    "    \n",
    "    # tensorboard callback\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir='tensorboard_log/' + current_time\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch, attribute_batch in dataset:\n",
    "            attribute_input, gen_loss, disc_loss, mean_real_output, mean_fake_output = train_step(image_batch, attribute_batch)\n",
    "\n",
    "        # Produce images for the GIF as you go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 1, attribute_input[:num_examples_to_generate])\n",
    "        \n",
    "        # save data for tensorboard\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('gen_loss', gen_loss, step=epoch)\n",
    "            tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n",
    "            tf.summary.scalar('mean_real_output', mean_real_output, step=epoch)\n",
    "            tf.summary.scalar('mean_fake_output', mean_fake_output, step=epoch)\n",
    "\n",
    "        # Save the model every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "    \n",
    "# Generate and save images\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False. This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "    image_folder = 'generated_images/'\n",
    "    image_filepath = image_folder + 'image_at_epoch_{:04d}.png'\n",
    "    plt.savefig(image_filepath.format(epoch))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fed0f9-335b-449a-b0a7-bce76e985722",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Generator and discriminator are trained simultaneously.  \n",
    "\n",
    "It is important that the generator and discriminator do not overpower each other (i.e. they train at a similar rate)\n",
    "\n",
    "At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits. This may take about one minute / epoch with the default settings on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819499f0-4d86-4b01-aeae-94b2a78456fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "EPOCHS = 200\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "train(ds_train, EPOCHS, num_examples_to_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8254a-327b-430a-b295-7c45ed401539",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743573b2-faad-4292-84af-a03d49aa6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d86c584-cdd8-44f9-a414-73b0c185c35a",
   "metadata": {},
   "source": [
    "### Generate images according to attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80efaec6-d7a9-4d25-a805-9b1b26f2b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(attributes.keys()))\n",
    "attributes.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4802e27-223e-4ca1-a827-115ff7f8aa16",
   "metadata": {},
   "source": [
    "attributes_bool_dict = {}\n",
    "\n",
    "for idx, attribute in enumerate(attributes.keys()):\n",
    "    attributes_dict = {}\n",
    "    for idx, attribute_child in enumerate(attributes.keys()):\n",
    "        attributes_dict[attribute_child] = False\n",
    "        attributes_dict[attribute] = True\n",
    "    attributes_bool_dict[attribute] = attributes_dict\n",
    "    \n",
    "#attributes_bool_dict"
   ]
  },
  {
   "cell_type": "raw",
   "id": "916c9fda-3000-450c-a356-7926d90f0938",
   "metadata": {},
   "source": [
    "# Ordered Values\n",
    "\n",
    "attribute_to_test = 'Eyeglasses'\n",
    "\n",
    "def attributes_bool_ordered(attribute_to_order):\n",
    "    \"\"\"\n",
    "    order boolean values in same order as received by model\n",
    "    output: boolean values\n",
    "    \"\"\"\n",
    "\n",
    "    attribute_values = []\n",
    "    for attribute in sorted(attributes_bool_dict[attribute_to_order].keys()):\n",
    "        attribute_values.append(attributes_bool_dict[attribute_to_order][attribute])\n",
    "        \n",
    "    return attribute_values\n",
    "    \n",
    "attribute_values = attributes_bool_ordered(attribute_to_test)\n",
    "print(len(attribute_values))\n",
    "print(type(attribute_values)) # list of true and false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087737c-c242-420f-93d5-a0413f33c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_base_dict = {}\n",
    "\n",
    "for idx, attribute in enumerate(attributes.keys()):\n",
    "    attribute_base_dict[attribute] = False\n",
    "\n",
    "#attribute_base_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75331ada-6a4c-4736-9cce-e41fbcf826fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attribute_to_test = ['Eyeglasses', 'Male', 'Attractive']\n",
    "\n",
    "def attributes_bool_ordered(attribute_base_dict, attribute_to_find):\n",
    "    \"\"\"\n",
    "    order boolean values in same order as received by model\n",
    "    output: boolean values\n",
    "    \"\"\"\n",
    "    attribute_to_find = list(attribute_to_find)\n",
    "    \n",
    "    attribute_values = []\n",
    "    for attribute in sorted(attribute_base_dict.keys()):\n",
    "        attribute_values.append(True if attribute in attribute_to_find else False)\n",
    "        \n",
    "    return attribute_values\n",
    "    \n",
    "#attribute_values = attributes_bool_ordered(attribute_base_dict, attribute_to_test)\n",
    "print(len(attribute_values))\n",
    "print(type(attribute_values)) # list of true and false\n",
    "#attribute_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b1112-4db8-4b9f-987d-869fe7162485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_multiple_list(attribute_to_find):\n",
    "    if any(isinstance(item, list) for item in attribute_to_find):\n",
    "        attribute_values = []\n",
    "        for attribute_list in attribute_to_find:\n",
    "            attribute_values.append(attributes_bool_ordered(attribute_base_dict, attribute_list))\n",
    "        \n",
    "    else:\n",
    "        attribute_values = attributes_bool_ordered(attribute_base_dict, attribute_to_find)\n",
    "    return attribute_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b623737a-6ba7-4ca8-a568-a57794834571",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_to_test = [['Eyeglasses', 'Male', 'Attractive'], ['Eyeglasses', 'Male', 'Attractive', 'Blond_Hair']]\n",
    "#attribute_to_test = ['Eyeglasses', 'Male', 'Attractive']\n",
    "attribute_to_test = attributes_multiple_list(attribute_to_test)\n",
    "#attribute_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856ba78-002f-4841-a983-c5a1204129fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stack(list(attribute_to_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00328f9-820c-4c62-ae54-af8cc5166022",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stack(list(attribute_to_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ebe567-d2f1-4843-82db-aca562a83dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_attrbutes_to_vec(attributes, attribute_embeddings):\n",
    "    \n",
    "    # Order attributes\n",
    "    #attributes = attributes_bool_ordered(attribute_base_dict, attributes)\n",
    "    attributes = attributes_multiple_list(attributes)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    print('p1: ',attribute_embeddings.shape, type(attribute_embeddings))\n",
    "    batch_attributes_as_bool_tensor = tf.stack(list(attributes))\n",
    "    print('p2: ',batch_attributes_as_bool_tensor.shape, type(batch_attributes_as_bool_tensor))\n",
    "    batch_attributes_as_bool_tensor = tf.reshape(batch_attributes_as_bool_tensor, [5,40])\n",
    "    print('p2: ',batch_attributes_as_bool_tensor.shape, type(batch_attributes_as_bool_tensor))\n",
    "    \n",
    "    # Convert from boolean to float\n",
    "    batch_attributes_as_float_tensor = tf.where(batch_attributes_as_bool_tensor, 1.,-1.)\n",
    "    print('p3', batch_attributes_as_float_tensor.shape, type(batch_attributes_as_float_tensor))\n",
    "    \n",
    "    # Create projection of attributes into embedding space\n",
    "    attribute_input = tf.matmul(batch_attributes_as_float_tensor, attribute_embeddings) \n",
    "    return attribute_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba9ecb3-fb68-45c8-8530-f5d77bce1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_to_test = ['Eyeglasses', 'Male', 'Attractive', 'Blond_Hair']\n",
    "#attribute_to_test = ['Eyeglasses', 'Male', 'Attractive']\n",
    "\n",
    "attributes_test = convert_attrbutes_to_vec(attribute_to_test, attribute_embeddings)\n",
    "print(attributes_test.shape, type(attributes_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce42ca9-fd48-468c-bfc4-82123093aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_to_test = ['Eyeglasses', 'Male', 'Attractive', 'Blond_Hair']\n",
    "#attribute_to_test = ['Eyeglasses', 'Male', 'Attractive']\n",
    "attribute_to_test = [['Eyeglasses', 'Male', 'Attractive'], ['Eyeglasses', 'Male', 'Attractive', 'Blond_Hair'], ['Eyeglasses', 'Male', 'Blond_Hair'], \n",
    "                     ['Attractive', 'Blond_Hair'],['Attractive', 'Blond_Hair', 'Wearing_Lipstick']]\n",
    "\n",
    "\n",
    "attributes_test = convert_attrbutes_to_vec(attribute_to_test, attribute_embeddings)\n",
    "print(attributes_test.shape, type(attributes_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb4b67-c259-4232-bd44-cbc3f05c66dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try generating an image based on specific attributes\n",
    "attributes_noise = initiate_seed_weights(attributes)\n",
    "\n",
    "generated_image = generator(attributes_test, training=False)\n",
    "print(generated_image.dtype)\n",
    "print(generated_image.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(generated_image[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "#plt.imshow(generated_image[1])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837a810-d061-4f96-a78a-588910ba4826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e29e3c-f3b5-4cbf-b544-18f7bddaef0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c4840-2bec-40c0-8367-c1f82093a305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da33b0-1140-4726-891d-8482c68f154d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a8c7ea-270a-4198-8f67-2a1077fdb126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd48a27-228b-4af1-9e95-049243ad4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "    return PIL.Image.open('./generated_images/image_at_epoch_{:04d}.png'.format(epoch_no))\n",
    "\n",
    "display_image(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6f73f-2f37-4640-bd83-7be7097a9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_random = np.random.choice(a=[False, True], size=(128,40))\n",
    "print(attributes_random[1].shape)\n",
    "attributes_random[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc4956-d857-4926-9ec2-303f6542a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4d3b4-fe28-46ab-adef-ba2a24173c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88071b1-f0b5-4303-abc7-2b0eac828d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
